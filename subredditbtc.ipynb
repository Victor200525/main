{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12863382,"sourceType":"datasetVersion","datasetId":8134722},{"sourceId":12875348,"sourceType":"datasetVersion","datasetId":8144896}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Очистать директорию OutPUT\n#!rm -rf /kaggle/working/*","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, sys\nimport json\nfrom datetime import datetime, date\nfrom pydantic import BaseModel\nfrom transformers import pipeline\nimport tiktoken","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install loguru\nfrom loguru import logger\nlogger.remove() # удаляет запись в консоль\n# Добавляем лог-файл (enqueue=True включает неблокирующую очередь)\nlogger.add(sys.stdout)\nlogger.add(\"async_log.log\", format=\"{time} | {level} | {message}\", rotation=\"1 MB\", enqueue=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Обновление последней версии polars\n#!pip install -U deltalake\nimport deltalake\n#!pip install -U polars  1.32.\nimport polars as pl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Подключение к Clouflare R2 S3 (необязательно)\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nACCESS_KEY = user_secrets.get_secret(\"ACCESS_KEY\")\nSECRET_KEY = user_secrets.get_secret(\"SECRET_KEY\")\nENDPOINT_URL = \"https://9cf55f33e92c95201664f2c62ca31641.r2.cloudflarestorage.com/\"\nstorage_options = {\n    \"AWS_ACCESS_KEY_ID\": ACCESS_KEY,\n    \"AWS_SECRET_ACCESS_KEY\": SECRET_KEY,\n    \"AWS_ENDPOINT_URL\": ENDPOINT_URL,  # например, https://s3.us-east-1.wasabisys.com\n    # Для MinIO/R2 безопасная конкуренция без DynamoDB:\n    \"aws_conditional_put\": \"etag\"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T15:29:57.729153Z","iopub.execute_input":"2025-08-26T15:29:57.729573Z","iopub.status.idle":"2025-08-26T15:29:58.225988Z","shell.execute_reply.started":"2025-08-26T15:29:57.729525Z","shell.execute_reply":"2025-08-26T15:29:58.224998Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Константы\nBATCH_SIZE = 25000\nINPUT_DIR = '/kaggle/input/subreddit-btc/'\nOUTPUT_DIR = '/tmp/stage'\n\nclass RedditItem(BaseModel):\n    \"\"\"Pydantic model for raw Reddit data\"\"\"\n    selftext: str\n    upvotes: int\n    num_of_comments: int\n    date_: date\n\nclass ProcessedItem(BaseModel):\n    \"\"\"Pydantic model for processed sentiment analysis results\"\"\"\n    text: str\n    upvotes: int\n    numofcomms: int\n    sentiment: float\n    date_: date  \n\nclass SentimentHuggingFaceModel:\n    def __init__(self):\n        self.sentiment_model = pipeline(\n                                \"sentiment-analysis\",\n                                model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n                                batch_size=16\n                                )\n\n    def get_sentiment(self, selftext: str) -> float:\n        \"\"\"Вычисляет тональность текста\"\"\"\n        try:\n            selftext = selftext[:500]\n            result = self.sentiment_model(selftext)\n            label = result[0][\"label\"].lower()\n            score = result[0][\"score\"]\n\n            if label == \"positive\":\n                return score\n            elif label == \"negative\":\n                return -score\n            else:\n                return 0\n        except Exception as e:\n            print(f\"Ошибка API: {e}\")\n            return 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Функции с основной логиков обработки\ndef read_and_process_files(sentiment_model, input_dir, output_dir, batch_size):\n    \"\"\"Читает файлы и обрабатывает их батчами\"\"\"\n    batch = []\n    \n    for filename in os.listdir(input_dir):\n        file_path = os.path.join(input_dir, filename)\n        if not os.path.isfile(file_path):\n            continue\n            \n        with open(file_path, 'r', encoding='utf-8') as file:\n            for line in file:\n                try:\n                    data = json.loads(line)\n                    selftext = data.get('selftext', '')\n                    \n                    if not selftext.strip() or selftext.lower() in {\"[deleted]\", \"[removed]\"}:\n                        continue\n                        \n                    # Create RedditItem instance\n                    item = RedditItem(\n                        selftext=selftext,\n                        upvotes=data.get('score', 0),\n                        num_of_comments=data.get('num_comments', 0),\n                        date_=datetime.utcfromtimestamp(int(data.get('created_utc', 0) or 0)).date()\n                    )\n                    \n                    batch.append(item)\n                    \n                    # Обрабатываем батч когда он заполнится\n                    if len(batch) >= batch_size:\n                        logger.info(f'Processing batch of {len(batch)} items')\n                        process_batch(sentiment_model, batch, output_dir)\n                        batch = []\n                        \n                except Exception as e:\n                    logger.error(f'Read file error: {e}, file: {file}')\n                    continue\n    \n    if batch:\n        logger.info(f'Processing final batch of {len(batch)} items')\n        try:\n            process_batch(sentiment_model, batch, output_dir)\n        except Exception as e:\n            logger.error(f\"Error processing final batch: {e}, file: {file}\")\n\ndef process_batch(sentiment_model, batch, output_dir):\n    \"\"\"Обрабатывает один батч и сохраняет\"\"\"\n    logger.info('Batch processing started')\n    \n    processed = []\n    for item in batch:\n        try:\n            sentiment = sentiment_model.get_sentiment(item.selftext)\n            \n            processed.append(ProcessedItem(\n                text=item.selftext,\n                upvotes=item.upvotes,\n                numofcomms=item.num_of_comments,\n                sentiment=sentiment,\n                date_=item.date_\n            ))\n        except Exception as e:\n            logger.error(f'Processing item error: {e}')\n            continue\n            \n    if processed:\n        logger.info(f'Batch processing finished, saving {len(processed)} items')\n        save_to_delta(processed, output_dir)\n\ndef save_to_delta(data, output_dir):\n    \"\"\"Сохраняет в Delta Lake\"\"\"\n    try:\n        # Convert ProcessedItem objects to dictionaries\n        data_dicts = [item.model_dump() for item in data]\n        df = pl.DataFrame(data_dicts)\n        df.write_delta(output_dir, mode=\"append\")\n        logger.info('Delta table saved')\n    except Exception as e:\n        logger.error(f'Save delta error: {e}')\n\ndef set_sentiment():\n    \"\"\"Основная функция\"\"\"\n    sentiment_model = SentimentHuggingFaceModel()\n    read_and_process_files(sentiment_model, INPUT_DIR, OUTPUT_DIR, BATCH_SIZE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !!!!! ЗАПУСК РАСЧЕТОВ !!!!\nset_sentiment()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Просмотреть log-и\nwith open('/kaggle/working/async_log.log', 'r') as f:\n    logs = f.read()\nprint(logs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Проверить количество строк в файле\nwith open('/kaggle/input/subreddit-btc/btc_part_aa') as f:\n    line_count = sum(1 for _ in f)\n    print(line_count)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Прочитать содержимое delta table\nDIR_ = '/kaggle/input/zip-outputs/'\n#DIR_ = '/kaggle/working/sentiment_data'\ndf = pl.read_delta(DIR_)\ndf","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Записать во внешнюю S3 таблицу \ntable_path = \"s3://bucket01/sentiment_data/\"\ndf.write_delta(table_path, storage_options=storage_options, mode=\"overwrite\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-26T15:30:12.483684Z","iopub.execute_input":"2025-08-26T15:30:12.483977Z","iopub.status.idle":"2025-08-26T15:31:18.219829Z","shell.execute_reply.started":"2025-08-26T15:30:12.483956Z","shell.execute_reply":"2025-08-26T15:31:18.218606Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"# copy from /tmp directory to Output (/kaggle/working)\n!cp -r /tmp/stage/* /kaggle/working/\n!cd /kaggle/working/ && zip -r ./output_files.zip .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Код если потом захотим копировать файлы из локальных папок удаленно\nimport boto3\nfrom botocore.exceptions import ClientError\n\ndef upload_to_s3(file_path, bucket_name, object_name=None):\n    \"\"\"\n    Upload a file to an S3 bucket\n    \n    :param file_path: Path to file to upload\n    :param bucket_name: Target S3 bucket name\n    :param object_name: S3 object name (defaults to file name)\n    :return: True if successful, False otherwise\n    \"\"\"\n    \n    # Initialize S3 client\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    \n    # Default object name to file name\n    if object_name is None:\n        object_name = os.path.basename(file_path)\n    \n    try:\n        # Upload with public read access\n        s3_client.upload_file(\n            file_path,\n            bucket_name,\n            object_name,\n            ExtraArgs={'ACL': 'public-read'}  # Remove if private upload needed\n        )\n        print(f\"Successfully uploaded {file_path} to s3://{bucket_name}/{object_name}\")\n        return True\n    except ClientError as e:\n        print(f\"Upload failed: {e}\")\n        return False\n    except FileNotFoundError:\n        print(f\"File not found: {file_path}\")\n        return False\n\n# Example usage:\n# upload_to_s3('data.csv', 'my-s3-bucket', 'kaggle-data/data.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}